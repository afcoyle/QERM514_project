---
title: "03_modeling_creation"
author: "Aidan Coyle"
date: "5/24/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In this script, we will be creating our models, along with giving some brief explanations for the choices made, and evaluating the created models

First, read in our data and load all necessary libraries

```{r libraries, warning = FALSE, message = FALSE}
# Add all required libraries here
list.of.packages <- c("tidyverse", "lme4", "MuMIn", "rcompanion", "MASS")
# Get names of all required packages that aren't installed
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[, "Package"])]
# Install all new packages
if(length(new.packages)) install.packages(new.packages)


# Load all required libraries
lapply(list.of.packages, FUN = function(X) {
  do.call("require", list(X))
})

# Load in our data file created in the previous script
crabdat <- read.csv("../data/allyears_cleaned.csv")
```

Now, let's review our model. We have a Bernoulli distribution, plus random effects, so we'll be making a generalized linear mixed model (or GLMM). 

Before doing anything else, we'll ensure all our variables were read in correctly (as either categorical or continuous predictors)

```{r}
# See class of each column
str(crabdat)
# Looks like we've got lots of columns that should be converted to factors!
crabdat$LOCATION_CODE <- factor(crabdat$LOCATION_CODE)
crabdat$SEX_CODE <- ordered(crabdat$SEX_CODE,
                            levels = c("M", "F"))
crabdat$SHELL_CONDITION_CODE <- ordered(crabdat$SHELL_CONDITION_CODE)
crabdat$BLACKMAT_CODE <- ordered(crabdat$BLACKMAT_CODE)
crabdat$PARASITE_CODE <- ordered(crabdat$PARASITE_CODE)
crabdat$LEG_CONDITION_CODE <- ordered(crabdat$LEG_CONDITION_CODE)

```

### Check for correlation

Prior to modeling anything, we're going to run checks on combinations of variables to see if any are correlated

```{r}
# CORRELATION BETWEEN CONTINUOUS VARIABLES

crabnums <- select_if(crabdat, is.numeric)
numcor <- cor(crabnums, method = "pearson")

# See the resulting table
print(numcor)

# See if any correlations are > 0.6 (our bar for correlation) and less than 1 (since every variable is perfectly correlated with itself)
any(abs(numcor) > 0.6 & numcor < 1)
# Looks like none are above our bar!

# CORRELATIONS BETWEEN CATEGORICAL VARIABLES

# Now we're using Cramer's V test to look at correlation among our categorical variables
crabcat <- select_if(crabdat, is.factor)

# Turn all from factors to numeric
crabcat[] <- sapply(crabcat, as.numeric)

# Initialize a blank matrix
results_matrix <- matrix(nrow = length(crabcat), ncol = length(crabcat))
# Name all rows and columns with our variable names
colnames(results_matrix) <- names(crabcat)
rownames(results_matrix) <- names(crabcat)

# Fill the matrix by performing Cramer's V test on each possible combination of factors
for (i in 1:ncol(crabcat)) {
  for (j in 1:ncol(crabcat)) {
    cramer.table <- table(crabcat[,i],crabcat[,j])
    cramer.matrix <- as.matrix(cramer.table)
    results_matrix[i,j] <- cramerV(cramer.matrix)
  }
}
# See the resulting matrix
print(results_matrix)

# See if any of our correlations (aside from self-correlations) cross our boundary of too much correlation
any(results_matrix > 0.6 & results_matrix < 1)
# Looks like none are above our bar

# CORRELATIONS BETWEEN CATEGORICAL AND CONTINUOUS VARIABLES

# We'll use Spearman rank-order correlation to determine whether we have any correlation
crabrank <- crabdat
crabrank[] <- sapply(crabdat, as.numeric)
crabcomps <- cor(crabrank, method = "spearman")
any(abs(crabcomps) > 0.6 & crabcomps < 1)
# Looks like we do have some significant correlations this time! Let's pull them out
which(abs(crabcomps) > 0.6 & crabcomps < 1, arr.ind = TRUE)
names(crabdat)[c(5,4)]
```
Looks like sex and carapace width are correlated! Therefore, we should only put one of these into our model. All other variables don't show sufficient correlation to cause problems.
      
### Scaling variables

We also need to scale some of our predictors to allow the model to fit more easily


```{r}
# Subtract the year before the earliest data from our survey, so year now starts at 1
crabdat$s.YEAR <- crabdat$YEAR-(min(crabdat$YEAR)-1)
# Scale depth, carapace width, and Julian day
crabdat$DEPTH_SCALED <- scale(crabdat$DEPTH_FATHOMS)
crabdat$WIDTH_SCALED <- scale(crabdat$WIDTH_MILLIMETERS)
crabdat$DAY_SCALED <- scale(crabdat$JUL_DAY)
```



## Modeling

#### Penalized Quasi-Likelihood

Now we're ready to model! First, we'll try to model with penalized quasi-likelihood. These are labeled pql_ [for penalized quasi-likelihood] ns/nw [for no sex or no width] full_mod

```{r}
# First, trying nearly full model with penalized quasi-likelihood. This one includes sex, but not width
pql_nwfull_mod <- glmmPQL(fixed = PARASITE_CODE ~ DEPTH_SCALED + DAY_SCALED + SEX_CODE + SHELL_CONDITION_CODE + BLACKMAT_CODE + LEG_CONDITION_CODE, random = list(s.YEAR = ~1, LOCATION_CODE = ~1), 
                        data = crabdat,
                        family = "binomial")
pql_nwfull_mod

pql_nsfull_mod <- glmmPQL(fixed = PARASITE_CODE ~ DEPTH_SCALED + DAY_SCALED + WIDTH_SCALED + SHELL_CONDITION_CODE + BLACKMAT_CODE + LEG_CONDITION_CODE, random = list(s.YEAR = ~1, LOCATION_CODE = ~1), 
                        data = crabdat,
                        family = "binomial")
pql_nsfull_mod
```
Both models ended up fairly similar - Black Mat has the strongest effect on parasite code (a negative effect, specifically), followed by shell condition. Then sex or carapace width have slight positive effects, depth has a slight negative effect, and leg condition has a negligible effect. 

However, this doesn't produce AIC values or give a way to compare our models. Therefore, we'll try to run this again, but with Laplace Approximation

#### Laplace Approximation

One difficulty with Laplace approximation is that it is slower and more computationally intensive. We have a very complex model, which, in fact, makes it impossible to run our full model immediately. As a result, instead we plan to create a smaller model and then slowly build on it using the update() function

```{r}
# Start by building small model with 2 fixed effects + our random effects. We will choose blackmat code and shell condition, since they had the strongest effect on our PQL model
lap_first_mod <- glmer(PARASITE_CODE ~ BLACKMAT_CODE + SHELL_CONDITION_CODE + (1 | LOCATION_CODE) + (1 | s.YEAR), 
                       data = crabdat, 
                       family = binomial, 
                       na.action = "na.fail",   # This chunk is for the dredge() function used later
                       control = glmerControl(optimizer = c("bobyqa"))) # This is to improve optimization

#Next, update the model to include the effects of depth
lap_two_mod <- update(lap_first_mod, PARASITE_CODE ~ BLACKMAT_CODE + SHELL_CONDITION_CODE + DEPTH_SCALED + (1 | LOCATION_CODE) + (1 | s.YEAR))

# Next, update the model to include the effects of day
lap_three_mod <- update(lap_two_mod, PARASITE_CODE ~ BLACKMAT_CODE + SHELL_CONDITION_CODE  + DEPTH_SCALED  + DAY_SCALED + (1 | LOCATION_CODE) + (1 | s.YEAR))

# Next, update the model to include the effects of leg condition
lap_four_mod <- update(lap_three_mod, PARASITE_CODE ~ BLACKMAT_CODE + SHELL_CONDITION_CODE + DEPTH_SCALED + DAY_SCALED + LEG_CONDITION_CODE + (1 | LOCATION_CODE) + (1 | s.YEAR))

# Now, we'll update with our correlated variables. One of the models will include sex, and the other will include carapace width. When we include LEG_CONDITION_CODE, lap_sex_mod DOES run, but lap_CW_mod returns an error. Since it had a negligible overall effect in our PQL models, we'll remove it in both
lap_sex_mod <- update(lap_four_mod, PARASITE_CODE ~ BLACKMAT_CODE + SHELL_CONDITION_CODE +  DEPTH_SCALED + DAY_SCALED + SEX_CODE + (1 | LOCATION_CODE) + (1 | s.YEAR))

lap_CW_mod <- update(lap_four_mod, PARASITE_CODE ~ BLACKMAT_CODE + SHELL_CONDITION_CODE + DEPTH_SCALED + DAY_SCALED + WIDTH_SCALED + (1 | LOCATION_CODE) + (1 | s.YEAR))


```

### Dredging

We will now use the dredge() function from the MuMIn package to go through each of our model possibilities and select an optimal model using AIC.

```{r}
# First, extract AIC of all models we just created
extractAIC(lap_first_mod)
extractAIC(lap_two_mod)
extractAIC(lap_three_mod)
extractAIC(lap_four_mod)
extractAIC(lap_sex_mod)
extractAIC(lap_CW_mod)

# Looks like the best model of these is lap_CW_mod, which includes all variables except sex. We will treat this as our "full model" going forward, 

# Use the dredge() function to generate a model selection table with various combinations of fixed effect terms
lap_mods <- dredge(lap_CW_mod, beta = "none",
       eval = TRUE,
       rank = "AICc")
# See all the resulting models
lap_mods

# Looks like only two of our models have weights > 0! Let's look at them:
best_two <- get.models(lap_mods, subset = weight > 0.01)

# First model - heaviest weight, at 0.72
first_mod <- best_two[[1]]
first_mod
summary(first_mod)

# Second model - contains the rest of the weight at 0.28
second_mod <- best_two[[2]]
second_mod
summary(second_mod)

# Average models based on AICc
avg_model <- model.avg(best_two, beta = "none")

# See what that average model looks like
avg_model
summary(avg_model)
```

### Model Diagnostics

Now that we have produced some models, before we accept their average, we need to do some diagnostics to ensure the models meet our assumptions

```{r}
# GOODNESS OF FIT

chisq <- sum((as.integer(as.character(crabdat$PARASITE_CODE)) - fitted(first_mod))^2 / fitted(first_mod))

pchisq(chisq, df = nrow(crabdat) - length(coef(first_mod)), lower.tail = FALSE)
# P-value is large, so don't reject null hypothesis for our first model. Repeat for second model

chisq <- sum((as.integer(as.character(crabdat$PARASITE_CODE)) - fitted(second_mod))^2 / fitted(second_mod))

pchisq(chisq, df = nrow(crabdat) - length(coef(second_mod)), lower.tail = FALSE)
# P-value is large, so don't reject null hypothesis for our second model

# Both models pass our goodness-of-fit test!

# Q-Q PLOT OF RESIDUALS

qqnorm(residuals(first_mod), main = "QQ plot (residuals)")
qqnorm(residuals(second_mod), main = "QQ plot (residuals)")
# Hmm, both Q-Q plots show sharp breaks in the middle. Some 

# Q-Q PLOT OF RANDOM EFFECTS

# PLOT OF RESIDUALS VS. FITTED\

# CHECK FOR OVERDISPERSION

k <- length(coef(first_mod)) + length(ranef(first_mod))
pchisq(deviance(first_mod), k, lower.tail = FALSE)

k <- length(coef(second_mod)) + length(ranef(second_mod))
pchisq(deviance(second_mod), k, lower.tail = FALSE)
# In both cases, looks like we DO have overdispersion! 

# Instead, let's fit to a negative binomial model
```

### Fitting to a Negative Binomial Model


```{r}
# Fitting our old global model that incorporates effect of all except sex and leg condition
nb_first_mod <- glmer.nb(as.numeric(as.character(PARASITE_CODE)) ~ BLACKMAT_CODE + SHELL_CONDITION_CODE + (1 | LOCATION_CODE) + (1 | s.YEAR), 
                       data = crabdat, 
                       family = binomial, 
                       na.action = "na.fail",   # This chunk is for the dredge() function used later
                       control = glmerControl(optimizer = c("bobyqa"))) # This is to improve optimization

lap_two_mod <- update(lap_first_mod, PARASITE_CODE ~ BLACKMAT_CODE + SHELL_CONDITION_CODE + DEPTH_SCALED + (1 | LOCATION_CODE) + (1 | s.YEAR))

# Next, update the model to include the effects of day
lap_three_mod <- update(lap_two_mod, PARASITE_CODE ~ BLACKMAT_CODE + SHELL_CONDITION_CODE  + DEPTH_SCALED  + DAY_SCALED + (1 | LOCATION_CODE) + (1 | s.YEAR))

# Next, update the model to include the effects of leg condition
lap_four_mod <- update(lap_three_mod, PARASITE_CODE ~ BLACKMAT_CODE + SHELL_CONDITION_CODE + DEPTH_SCALED + DAY_SCALED + LEG_CONDITION_CODE + (1 | LOCATION_CODE) + (1 | s.YEAR))

# Now, we'll update with our correlated variables. One of the models will include sex, and the other will include carapace width. When we include LEG_CONDITION_CODE, lap_sex_mod DOES run, but lap_CW_mod returns an error. Since it had a negligible overall effect in our PQL models, we'll remove it in both
lap_sex_mod <- update(lap_four_mod, PARASITE_CODE ~ BLACKMAT_CODE + SHELL_CONDITION_CODE +  DEPTH_SCALED + DAY_SCALED + SEX_CODE + (1 | LOCATION_CODE) + (1 | s.YEAR))

lap_CW_mod <- update(lap_four_mod, PARASITE_CODE ~ BLACKMAT_CODE + SHELL_CONDITION_CODE + DEPTH_SCALED + DAY_SCALED + WIDTH_SCALED + (1 | LOCATION_CODE) + (1 | s.YEAR))

```

