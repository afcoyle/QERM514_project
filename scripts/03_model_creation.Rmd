---
title: "03_modeling_creation"
author: "Aidan Coyle"
date: "5/24/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In this script, we will be creating our models, along with giving some brief explanations for the choices made, and evaluating the created models

First, read in our data and load all necessary libraries

```{r libraries, warning = FALSE, message = FALSE}
# Add all required libraries here
list.of.packages <- c("tidyverse", "lme4", "MuMIn", "rcompanion", "MASS", "viridis", "generalhoslem", "mgcv")
# Get names of all required packages that aren't installed
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[, "Package"])]
# Install all new packages
if(length(new.packages)) install.packages(new.packages)


# Load all required libraries
lapply(list.of.packages, FUN = function(X) {
  do.call("require", list(X))
})

# Load in our data file created in the previous script
crabdat <- read.csv("../output/crab_data_tables/allyears_cleaned.csv")
```

Now, let's review our model. We have a Bernoulli distribution, plus random effects, so we'll be making a generalized linear mixed model (or GLMM). 

Before doing anything else, we'll ensure all our variables were read in correctly (as either categorical or continuous predictors)

```{r}
# See class of each column
str(crabdat)
# Looks like we've got lots of columns that should be converted to factors!
crabdat$LOCATION_CODE <- factor(crabdat$LOCATION_CODE)
crabdat$SEX_CODE <- ordered(crabdat$SEX_CODE,
                            levels = c("M", "F"))
crabdat$SHELL_CONDITION_CODE <- ordered(crabdat$SHELL_CONDITION_CODE)
crabdat$BLACKMAT_CODE <- ordered(crabdat$BLACKMAT_CODE)
crabdat$PARASITE_CODE <- ordered(crabdat$PARASITE_CODE)
crabdat$LEG_CONDITION_CODE <- ordered(crabdat$LEG_CONDITION_CODE)

```

### Check for correlation

Prior to modeling anything, we're going to run checks on combinations of variables to see if any are correlated

```{r}
# CORRELATION BETWEEN CONTINUOUS VARIABLES

crabnums <- select_if(crabdat, is.numeric)
numcor <- cor(crabnums, method = "pearson")

# See the resulting table
print(numcor)

# See if any correlations are > 0.6 (our bar for correlation) and less than 1 (since every variable is perfectly correlated with itself)
any(abs(numcor) > 0.6 & numcor < 1)
# Looks like none are above our bar!

# CORRELATIONS BETWEEN CATEGORICAL VARIABLES

# Now we're using Cramer's V test to look at correlation among our categorical variables
crabcat <- select_if(crabdat, is.factor)

# Turn all from factors to numeric
crabcat[] <- sapply(crabcat, as.numeric)

# Initialize a blank matrix
results_matrix <- matrix(nrow = length(crabcat), ncol = length(crabcat))
# Name all rows and columns with our variable names
colnames(results_matrix) <- names(crabcat)
rownames(results_matrix) <- names(crabcat)

# Fill the matrix by performing Cramer's V test on each possible combination of factors
for (i in 1:ncol(crabcat)) {
  for (j in 1:ncol(crabcat)) {
    cramer.table <- table(crabcat[,i],crabcat[,j])
    cramer.matrix <- as.matrix(cramer.table)
    results_matrix[i,j] <- cramerV(cramer.matrix)
  }
}
# See the resulting matrix
print(results_matrix)

# See if any of our correlations (aside from self-correlations) cross our boundary of too much correlation
any(results_matrix > 0.6 & results_matrix < 1)
# Looks like none are above our bar

# CORRELATIONS BETWEEN CATEGORICAL AND CONTINUOUS VARIABLES

# We'll use Spearman rank-order correlation to determine whether we have any correlation
crabrank <- crabdat
crabrank[] <- sapply(crabdat, as.numeric)
crabcomps <- cor(crabrank, method = "spearman")
any(abs(crabcomps) > 0.6 & crabcomps < 1)
# Looks like we do have some significant correlations this time! Let's pull them out
which(abs(crabcomps) > 0.6 & crabcomps < 1, arr.ind = TRUE)
names(crabdat)[c(5,4)]
```
Looks like sex and carapace width are correlated! Therefore, we should only put one of these into our model. All other variables don't show sufficient correlation to cause problems.
      
### Scaling variables

We also need to scale some of our predictors to allow the model to fit more easily


```{r}
# Subtract the year before the earliest data from our survey, so year now starts at 1
crabdat$s.YEAR <- crabdat$YEAR-(min(crabdat$YEAR)-1)
# Scale depth, carapace width, and Julian day
crabdat$DEPTH_SCALED <- scale(crabdat$DEPTH_FATHOMS)
crabdat$WIDTH_SCALED <- scale(crabdat$WIDTH_MILLIMETERS)
crabdat$DAY_SCALED <- scale(crabdat$JUL_DAY)
```



## Modeling

#### Penalized Quasi-Likelihood

Now we're ready to model! First, we'll try to model with penalized quasi-likelihood. These are labeled pql_ [for penalized quasi-likelihood] ns/nw [for no sex or no width] full_mod

```{r}
# First, trying nearly full model with penalized quasi-likelihood. This one includes sex, but not width
pql_nwfull_mod <- glmmPQL(fixed = PARASITE_CODE ~ DEPTH_SCALED + DAY_SCALED + SEX_CODE + SHELL_CONDITION_CODE + BLACKMAT_CODE + LEG_CONDITION_CODE, random = list(s.YEAR = ~1, LOCATION_CODE = ~1), 
                        data = crabdat,
                        family = "binomial")
summary(pql_nwfull_mod)

pql_nsfull_mod <- glmmPQL(fixed = PARASITE_CODE ~ DEPTH_SCALED + DAY_SCALED + WIDTH_SCALED + SHELL_CONDITION_CODE + BLACKMAT_CODE + LEG_CONDITION_CODE, random = list(s.YEAR = ~1, LOCATION_CODE = ~1), 
                        data = crabdat,
                        family = "binomial")
pql_nsfull_mod
```
Both models ended up fairly similar - Black Mat has the strongest effect on parasite code (a negative effect, specifically), followed by shell condition. Then sex or carapace width have slight positive effects, depth has a slight negative effect, and leg condition has a negligible effect. 

However, this doesn't produce AIC values or give a way to compare our models. Therefore, we'll try to run this again, but with Laplace Approximation


#### Determining What Variables to Include using Laplace Approximation

Our full model is too large to directly run. To determine what variables to include in our full model, we will make a Laplace Approximation model with each variable individually plus the random effects, and then compare that to a null model that contains just the random effects. Then we will use AIC to determine whether the variable improves the model over the null model. If so, we will include it in the full model.

```{r}
# Create null model and get AIC
null_mod <- glmer(PARASITE_CODE ~ (1 | LOCATION_CODE) + (1 | s.YEAR), 
                  data = crabdat,
                  family = binomial)
AIC_null <- extractAIC(null_mod)[2]
print(AIC_null)

# Create model with only Black Mat code
blackmat_mod <- glmer(PARASITE_CODE ~ BLACKMAT_CODE + (1 | LOCATION_CODE) + (1 | s.YEAR),
                      data = crabdat,
                      family = binomial)
extractAIC(blackmat_mod)
extractAIC(blackmat_mod)[2] < AIC_null
# Looks like Black Mat improves the model!

depth_mod <- glmer(PARASITE_CODE ~ DEPTH_SCALED + (1 | LOCATION_CODE) + (1 | s.YEAR),
                      data = crabdat,
                      family = binomial)
extractAIC(depth_mod)
extractAIC(depth_mod) < AIC_null
# Looks like depth also improves the model!

shellcond_mod <- glmer(PARASITE_CODE ~ SHELL_CONDITION_CODE + (1 | LOCATION_CODE) + (1 | s.YEAR),
                      data = crabdat,
                      family = binomial)
extractAIC(shellcond_mod)
extractAIC(shellcond_mod) < AIC_null
# Shell condition also improves the model!

day_mod <- glmer(PARASITE_CODE ~ DAY_SCALED + (1 | LOCATION_CODE) + (1 | s.YEAR),
                      data = crabdat,
                      family = binomial)
extractAIC(day_mod)
extractAIC(day_mod) < AIC_null
# Day also improves the model (though just barely)

CW_mod <- glmer(PARASITE_CODE ~ WIDTH_SCALED + (1 | LOCATION_CODE) + (1 | s.YEAR),
                      data = crabdat,
                      family = binomial)
extractAIC(CW_mod)
extractAIC(CW_mod) < AIC_null
# Carapace width also improves the model

sex_mod <- glmer(PARASITE_CODE ~ SEX_CODE + (1 | LOCATION_CODE) + (1 | s.YEAR),
                      data = crabdat,
                      family = binomial)
extractAIC(sex_mod)
extractAIC(sex_mod) < AIC_null

legcond_mod <- glmer(PARASITE_CODE ~ LEG_CONDITION_CODE + (1 | LOCATION_CODE) + (1 | s.YEAR),
                      data = crabdat,
                      family = binomial)
extractAIC(legcond_mod)
extractAIC(legcond_mod) < AIC_null
# Well shoot, leg condition also improves over the null model


```



#### Laplace Approximation

One difficulty with Laplace approximation is that it is slower and more computationally intensive. We have a very complex model, which, in fact, makes it impossible to run our full model immediately. As a result, instead we plan to create a smaller model and then slowly build on it using the update() function

```{r}
# Start by building small model with 2 fixed effects + our random effects. We will choose blackmat code and shell condition, since they had the strongest effect on our PQL model
lap_first_mod <- glmer(PARASITE_CODE ~ BLACKMAT_CODE + SHELL_CONDITION_CODE + (1 | LOCATION_CODE) + (1 | s.YEAR), 
                       data = crabdat, 
                       family = binomial, 
                       na.action = "na.fail",   # This chunk is for the dredge() function used later
                       control = glmerControl(optimizer = c("bobyqa"))) # This is to improve optimization

#Next, update the model to include the effects of depth
lap_two_mod <- update(lap_first_mod, PARASITE_CODE ~ BLACKMAT_CODE + SHELL_CONDITION_CODE + DEPTH_SCALED + (1 | LOCATION_CODE) + (1 | s.YEAR))

# Next, update the model to include the effects of day
lap_three_mod <- update(lap_two_mod, PARASITE_CODE ~ BLACKMAT_CODE + SHELL_CONDITION_CODE  + DEPTH_SCALED  + DAY_SCALED + (1 | LOCATION_CODE) + (1 | s.YEAR))

# Now, we'll update with our correlated variables. One of the models will include sex, and the other will include carapace width. We're adding our correlated variables followed by leg condition rather than adding leg condition first because doing the reverse simply won't run.

lap_sex_mod <- update(lap_three_mod, PARASITE_CODE ~ BLACKMAT_CODE + SHELL_CONDITION_CODE +  DEPTH_SCALED + DAY_SCALED + SEX_CODE + (1 | LOCATION_CODE) + (1 | s.YEAR))

lap_sexlegs_mod <- update(lap_sex_mod, PARASITE_CODE ~ BLACKMAT_CODE + SHELL_CONDITION_CODE +  DEPTH_SCALED + DAY_SCALED + SEX_CODE + LEG_CONDITION_CODE + (1 | LOCATION_CODE) + (1 | s.YEAR))

lap_CW_mod <- update(lap_three_mod, PARASITE_CODE ~ BLACKMAT_CODE + SHELL_CONDITION_CODE + DEPTH_SCALED + DAY_SCALED + WIDTH_SCALED + (1 | LOCATION_CODE) + (1 | s.YEAR))

lap_CWlegs_mod <- update(lap_CW_mod, PARASITE_CODE ~ BLACKMAT_CODE + SHELL_CONDITION_CODE + DEPTH_SCALED + DAY_SCALED + WIDTH_SCALED + LEG_CONDITION_CODE + (1 | LOCATION_CODE) + (1 | s.YEAR))

```

### Dredging

We will now use the dredge() function from the MuMIn package to go through each of our model possibilities and select an optimal full model using AIC.

```{r}
# First, extract AIC of our two full models
extractAIC(lap_sexlegs_mod)
extractAIC(lap_CWlegs_mod)

# Also extract AIC of our near-full models that exclude leg condition, since it causes problems when trying to use dredge()
extractAIC(lap_sex_mod)
extractAIC(lap_CW_mod)

# Of these, it looks like models that include CW are much better than models that include sex. Therefore, the best of our two full models is lap_CWlegs_mod, which includes all variables except sex.

# However, when trying to run dredge() on lap_CWlegs_mod, a very large number of the models fail to converge, returning "unable to evaluate scaled gradientModel failed to converge: degenerate  Hessian with 1 negative eigenvalues variance-covariance matrix computed from finite-difference Hessian is not positive definite or contains NA values: falling back to var-cov estimated from RXvariance-covariance matrix computed from finit"

# Therefore, we are forced to choose a "full" model that actually runs. Excluding Julian day does nothing to help it run, but excluding leg condition does let it run. Therefore, we will treat our new "full" model as one which includes all variables but sex and leg condition.

# Use the dredge() function to generate a model selection table with various combinations of fixed effect terms
lap_mods <- dredge(lap_CW_mod, beta = "none",
       eval = TRUE,
       rank = "AICc")
# See all the resulting models
lap_mods

# Looks like only two of our models have weights > 0! Let's look at them:
best_two <- get.models(lap_mods, subset = weight > 0.01)

# First model - heaviest weight, at 0.72
first_mod <- best_two[[1]]
first_mod
summary(first_mod)

# Second model - contains the rest of the weight at 0.28
second_mod <- best_two[[2]]
second_mod
summary(second_mod)

# Average models based on AICc
avg_model <- model.avg(best_two, beta = "none")

# See what that average model looks like
avg_model$coefficients
summary(avg_model)
```

### Model Diagnostics

Now that we have produced some models, before we accept their average, we need to do some diagnostics to ensure the models meet our assumptions

```{r}
# GOODNESS OF FIT

chisq <- sum((as.integer(as.character(crabdat$PARASITE_CODE)) - fitted(first_mod))^2 / fitted(first_mod))

pchisq(chisq, df = nrow(crabdat) - length(coef(first_mod)), lower.tail = FALSE)
# P-value is large, so don't reject null hypothesis for our first model. Repeat for second model

chisq <- sum((as.integer(as.character(crabdat$PARASITE_CODE)) - fitted(second_mod))^2 / fitted(second_mod))

pchisq(chisq, df = nrow(crabdat) - length(coef(second_mod)), lower.tail = FALSE)
# P-value is large, so don't reject null hypothesis for our second model

# Both models pass our goodness-of-fit test!

# Q-Q PLOT OF RESIDUALS

qqnorm(residuals(first_mod), main = "QQ plot (residuals)")
qqnorm(residuals(second_mod), main = "QQ plot (residuals)")

# Hmm, both Q-Q plots show sharp breaks in the middle, but not an issue for binary data

# CHECK WHETHER VARIATION IS ACCOUNTED FOR USING HOSMER-LEMESHOW

logitgof(crabdat$PARASITE_CODE, fitted(first_mod), g = 10)

logitgof(crabdat$PARASITE_CODE, fitted(second_mod), g = 10)

# In both cases, looks like we DO have variation that isn't accounted for, indicating we're missing parameters in our model. Disappointing, but we included all possible parameters in our model initially
```

### Fitting to a Negative Binomial Model


```{r}
# First, convert response variable to be numeric
crabdat$PARASITE_CODE <- as.numeric(as.character(crabdat$PARASITE_CODE))

# Fitting our old global model that incorporates effect of all except sex and leg condition
nb_first_mod <- glmer.nb(PARASITE_CODE ~ BLACKMAT_CODE + SHELL_CONDITION_CODE + (1 | LOCATION_CODE) + (1 | s.YEAR), 
                       data = crabdat, 
                       family = binomial, 
                       na.action = "na.fail",   # This chunk is for the dredge() function used later
                       control = glmerControl(optimizer = c("bobyqa"))) # This is to improve optimization

#Next, update the model to include the effects of depth
nb_two_mod <- update(nb_first_mod, PARASITE_CODE ~ BLACKMAT_CODE + SHELL_CONDITION_CODE + DEPTH_SCALED + (1 | LOCATION_CODE) + (1 | s.YEAR))

# Next, update the model to include the effects of day
nb_three_mod <- update(nb_two_mod, PARASITE_CODE ~ BLACKMAT_CODE + SHELL_CONDITION_CODE  + DEPTH_SCALED  + DAY_SCALED + (1 | LOCATION_CODE) + (1 | s.YEAR))

# Next, update the model to include the effects of carapace width. Our negative binomial model now includes all the variables from our full average model!

nb_CW_mod <- update(nb_three_mod, PARASITE_CODE ~ BLACKMAT_CODE + SHELL_CONDITION_CODE + DEPTH_SCALED + DAY_SCALED + WIDTH_SCALED + (1 | LOCATION_CODE) + (1 | s.YEAR))

nb_mods <- dredge(nb_CW_mod, beta = "none",
       eval = TRUE,
       rank = "AICc")
# See all the resulting models
nb_mods

# Again, looks like only two of our models have weights > 0 (though the weights are a bit different from our original model)! Let's look at them:
best_nbs <- get.models(nb_mods, subset = weight > 0.01)

# First model - heaviest weight, at 0.72
first_nb <- best_nbs[[1]]
first_nb
summary(first_nb)

# Second model - contains the rest of the weight at 0.28
second_nb <- best_nbs[[2]]
second_nb
summary(second_nb)

# Average models based on AICc
avg_nb <- model.avg(best_nbs, beta = "none")

# See what that average model looks like
avg_nb$coefficients
summary(avg_nb)
```


```{r graph_creation}
# Shell condition and Hematodinium
ggplot(crabdat, aes(fill = PARASITE_CODE, x = SHELL_CONDITION_CODE)) +
  geom_bar(position = "fill") +
  labs(x = "Shell condition",
       y = "Proportion") +
  scale_x_discrete(labels = c("Fresh", "New", "Old", "Very old")) +
  scale_fill_manual(values = viridis(2),
                    name = "Hematodinium",
                    labels = c("Uninfected", "Infected"))



# Depth and Hematodinium
ggplot(crabdat, aes(x = PARASITE_CODE, y = DEPTH_FATHOMS)) +
  geom_violin() +
  scale_y_reverse() +
  labs(x = "Hematodinium", 
       y = "Depth (fathoms)") +
  scale_x_discrete(labels = c("Uninfected", "Infected"))
  theme_minimal()
  
# Carapace Width  and Hematodinium
ggplot(crabdat, aes(x = PARASITE_CODE, y = WIDTH_MILLIMETERS)) +
  geom_violin() +
  labs(x = "Hematodinium", 
       y = "Carapace width (mm)") +
  scale_x_discrete(labels = c("Uninfected", "Infected"))
  theme_minimal()
  

  


# Black Mat and Hematodinium
ggplot(crabdat, aes(fill = PARASITE_CODE, x = BLACKMAT_CODE)) +
  geom_bar(position = "fill") +
  labs(x = "Black Mat",
       y = "Proportion") +
  scale_x_discrete(labels = c("Uninfected", "Infected")) +
  scale_fill_manual(values = viridis(2),
                    name = "Hematodinium",
                    labels = c("Uninfected", "Infected"))
  


nrow(crabdat[crabdat$PARASITE_CODE==1,])

```


